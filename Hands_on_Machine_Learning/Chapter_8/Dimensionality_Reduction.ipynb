{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM01ReH3t8JHVDbrB2jck4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matinsalami/DataScience/blob/main/Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Approaches for Dimensionality Reduction\n"
      ],
      "metadata": {
        "id": "En7kzQAVFnht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projection\n",
        "\n",
        "Although the instances have too many feaatures, most of the features are correlated and some others are constants. So, we can figure out that the all the instances lie in a lower-dimensional space which is a subset of the higher-dimensional space.\n",
        "\n",
        "Example: Most of the instances in a 3-D space are close to a 2-D plane which is a lower-Dimensionality space.\n",
        "                                                                                             "
      ],
      "metadata": {
        "id": "hmZ0-vmxIh3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold Learning\n",
        "\n",
        "Projection is not the optimal solution in some cases. Sometimes the subspace may twist and turn. We have this in Swiss roll toy datset.\n",
        "\n",
        "Projection in this case may squash different layers. So we should unroll the Swiss roll to obtain the 2D dataset."
      ],
      "metadata": {
        "id": "4eNCpTR_MCya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA\n",
        "\n",
        "PCA by far is the most popular dimensionality reduction algorithm. First it identifies the hyperplane closest to data, then it projects the data onto it."
      ],
      "metadata": {
        "id": "0CSbl8rBP0au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preserving the variance\n",
        "\n",
        "Before projecting onto a hyperplane, we must be able to find the hyperplane itself!\n",
        "\n",
        "We should be careful to choose a hyperplane that preserves the **maximum** amount of variance in the data set, otherwise we may loose a lot of information. Another way to justify our choice of hyperplane is to find a hyperplane to **minimize** the mean square error of the data.\n",
        "\n",
        "But what is **prinipal component**?\n",
        "\n"
      ],
      "metadata": {
        "id": "RkYE_CgJxze1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFgOvj95MHVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
