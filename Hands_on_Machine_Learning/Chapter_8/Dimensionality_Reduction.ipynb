{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO151SJsrdKFCWxAg18GsrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matinsalami/DataScience/blob/main/Hands_on_Machine_Learning/Chapter_8/Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Approaches for Dimensionality Reduction\n"
      ],
      "metadata": {
        "id": "En7kzQAVFnht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection\n",
        "\n",
        "Although the instances have too many feaatures, most of the features are correlated and some others are constants. So, we can figure out that the all the instances lie in a lower-dimensional space which is a subset of the higher-dimensional space.\n",
        "\n",
        "Example: Most of the instances in a 3-D space are close to a 2-D plane which is a lower-Dimensionality space.\n",
        "                                                                                             "
      ],
      "metadata": {
        "id": "hmZ0-vmxIh3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold Learning\n",
        "\n",
        "Projection is not the optimal solution in some cases. Sometimes the subspace may twist and turn. We have this in Swiss roll toy datset.\n",
        "\n",
        "Projection in this case may squash different layers. So we should unroll the Swiss roll to obtain the 2D dataset."
      ],
      "metadata": {
        "id": "4eNCpTR_MCya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA\n",
        "\n",
        "PCA by far is the most popular dimensionality reduction algorithm. First it identifies the hyperplane closest to data, then it projects the data onto it."
      ],
      "metadata": {
        "id": "0CSbl8rBP0au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preserving the variance\n",
        "\n",
        "Before projecting onto a hyperplane, we must be able to find the hyperplane itself!\n",
        "\n",
        "We should be careful to choose a hyperplane that preserves the **maximum** amount of variance in the data set, otherwise we may loose a lot of information. Another way to justify our choice of hyperplane is to find a hyperplane to **minimize** the mean square error of the data.\n",
        "\n",
        "But what is **prinipal component**?\n",
        "\n"
      ],
      "metadata": {
        "id": "RkYE_CgJxze1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Components\n",
        "\n",
        "The main objective of PCA is to find the first axis that accounts for the amount of variance in the training set. And also the second axis, orthogonal to the first one, that accounts for the largest amount of the remaining variance. The ith axis is called the ith principal component of the data. So how to find the principal components of a training set?\n",
        "\n",
        "We can use the matrix factorization technique called SVD or *singular value decomposition*. After that we will have 3 matrices, $\\mathbf{U} \\, \\mathbf{\\Sigma} \\, \\mathbf{V}^\\top$, where $\\mathbf{V}$ contains all the unit veccors that define the principal components that we are looking for.\n"
      ],
      "metadata": {
        "id": "p0poHA_A-THo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projecting Down to d dimensions\n",
        "\n",
        "Once all the PCs are defined, we can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane can ensure the highest variance in the data after projection. To do the projection, we will do the multiplication of X which is oringinal matrix of features by **Wd** which contains the first d columns of  $\\mathbf{V}$. With this method it is possible to reduce the dimensionality into any dimension in any dataset."
      ],
      "metadata": {
        "id": "tnuTEp0pCcSF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFgOvj95MHVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}